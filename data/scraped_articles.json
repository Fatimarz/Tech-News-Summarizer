[
    {
        "Title": "GNOME's new AI assistant can even run Linux commands for you - here's how",
        "Article": "There's a new AI assistant available for the GNOME desktop, and it just reached version 1.0 status. That new AI assistant is called Newelle, and it's already proven to be a worthy contender for your desktop.\n\nNewelle isn't just another large language model manager, but a full-blown assistant that can run Linux commands from human-readable descriptions (more on that in a bit), serve as a traditional AI chatbot, and more. Newelle uses Bai Chat as its backend and allows you to download and select from different LLMs (some of which will require an API key).\n\nAlso:\u00a0The top 5 GNOME extensions I install first (and what they can do for you)\n\nNewelle features chat history, chat editing, profiles, a mini app, extensions, keyboard shortcuts, chat save, recording, and more.\n\nThis app can easily serve as a missing link to add something similar to what Gemini is to Android (although it does take a bit more work to get it there).\n\nI've been using Newelle for a few days now and have found it to be quite a handy app. In fact, it's replaced Ollama/Msty as my go-to GUI for local LLMs. Not only is Newelle as easy to use as Ollama/Msty, it better fits the GNOME aesthetic, is faster, and doesn't take nearly the system resources.\n\nThe only caveat I've found with Newelle is that getting certain commands to run properly can be a challenge. There are specific steps you must take to allow commands to run, but once you've taken care of those configurations, you can use Newelle to run commands on your Linux system.\n\nLet's install Newelle and then configure it to run commands.\n\nWhat you'll need:\u00a0Newelle is installed on Linux via Flatpak, so you'll need a distribution with that universal package manager installed and working.\n\nThe first step is to open a terminal window. You can use any terminal window you have installed.\n\nTo install Newelle, run the following command:\n\nflatpak install https://dl.flathub.org/repo/appstream/io.github.qwersyk.Newelle.flatpakref\n\nWhen the command finishes, you can close the terminal window with the exit command.\n\nAlso:\u00a0How I feed my files to a local AI for better, more relevant responses\n\nBefore you configure Newelle to run commands, you'll need to select which large language model (LLM) to use. Some LLMs are easy to add, while others require an API key to function properly. Let's make this easy and set up a local LLM. Here's how.\n\nOpen the Newelle app from your desktop menu.\n\nFrom the Newelle main window, click the three horizontal line menu button near the top left and select Settings.\n\nThe Newelle UI is well designed and easy to use.\n\nFrom the LLM listing, click the downward-pointing arrow to download an LLM and then, once it's downloaded, select it by clicking the associated radio button. You can download as many LLMs as you want, but obviously, you can only use one at a time. Once you've taken care of this, you can close the Settings window.\n\nAlso:\u00a0I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\n\nYou can download as many LLMs as needed, but you can only use one at a time.\n\nThis is where it gets a bit tricky. You have to change a few specific bits to enable Newelle to run commands for you.\n\nGo back to the Settings window and click on the General tab.\n\nUnder Neural Network Control, disable \"Command virtualization.\"\n\nThis is the only configuration you must make in Newelle so commands can be run from the app.\n\nBecause Newelle is installed within a sandboxed environment, you have to give it permission to access your file system. To do that, you must install Flatseal with the command:\n\nflatpak install flathub com.github.tchx84.Flatseal\n\nOpen Flatseal and click on Newelle in the left sidebar. Scroll down until you see the Filesystem section. In that section, enable \"All user files.\"\n\nThis is the first configuration you must make in Flatseal.\n\nScroll down to the System Bus section and click the + button associated with Talks. In the new field, add:\n\norg.freedesktop.Flatpak\n\nThis is the final configuration you must make so Newelle can run Linux commands.\n\nYou can now close Flatseal.\n\nOnce you've taken care of the above, you can then run commands within Newelle. For instance, in the chat field, type:\n\ncreate a folder in my home directory named ZDNET\n\nNewelle will create the folder, and it's ready to use.\n\nAlso:\u00a0How I made Perplexity AI the default search engine in my browser (and why you should too)\n\nAnd that's the gist of Newelle. Using it as a traditional AI chatbot is straightforward and simple enough that anyone can use it. Sure, getting it to run commands takes a few extra steps, but if you're using Linux, you should be OK with that process.\n\nGet the morning's top stories in your inbox each day with our\u00a0Tech Today newsletter.",
        "Link": "https://www.zdnet.com/article/gnomes-new-ai-assistant-can-even-run-linux-commands-for-you-heres-how/",
        "Summary": "Newelle is a full-blown AI assistant for the GNOME desktop. It can run Linux commands from human-readable descriptions. Newelle uses Bai Chat as its backend and allows you to download and select from different LLMs. It features chat history, chat editing, profiles, a mini app, extensions, keyboard shortcuts, chat save, recording, and more. But getting certain commands to run properly can be a challenge, so be careful with your settings and install the right distribution. It's installed on Linux via Flatpak so you'll need a distribution with that universal package manager installed."
    },
    {
        "Title": "My go-to LLM tool just dropped a super simple Mac and PC app for local AI - why you should try it",
        "Article": "If you use AI, there are several reasons why you would want to work with it locally instead of from the cloud.\n\nFirst, it offers much more privacy. When using a Large Language Model (LLM) in the cloud, you never know if your queries or results are being tracked or even saved by a third party. Also, using an LLM locally saves energy. The amount of energy required to use a cloud-based LLM is growing and could be a problem in the future.\n\nErgo, locally hosted LLMs.\n\nAlso: How to run DeepSeek AI locally to protect your privacy \u2013 2 easy ways\n\nOllama is a tool that allows you to run different LLMs. I've been using it for some time and have found it to simplify the process of downloading and using various models. Although it does require serious system resources (you wouldn't want to use it on an aging machine), it does run fast, and allows you to use different models.\n\nBut Ollama by itself has been a command-line-only affair. There are some third-party GUIs (such as Msty, which has been my go-to). Until now, the developers behind Ollama hadn't produced their own GUI.\n\nThat all changed recently, and there's now a straightforward, user-friendly GUI, aptly named Ollama.\n\nThe GUI is fairly basic, but it's designed so that anyone can jump in right away and start using it. There is also a short list of LLMs that can easily be pulled from the LLM drop-down list. Those models are fairly common (such as the Gemma, DeepSeek, and Qwen models). Select one of those models, and the Ollama GUI will pull it for you.\n\nIf you want to use a model not listed, you would have to pull it from the command line like so:\n\nollama pull MODEL\n\nWhere MODEL is the name of the model you want.\n\nAlso: How I feed my files to a local AI for better, more relevant responses\n\nYou can find a full list of available models in the Ollama Library.\n\nAfter you've pulled a model, it appears in the drop-down to the right of the query bar.\n\nThe Ollama app is as easy to use as any cloud-based AI interface on the market, and it's free to use for MacOS and Windows (sadly, there's no Linux version of the GUI).\n\nI've kicked the tires of the Ollama app and found that, although it doesn't have quite the feature set of Msty, it's easier to use and fits in better with the MacOS aesthetic. The Ollama app also seems to be a bit faster than Msty (in both opening and responding to queries), which is a good thing because local AI can often be a bit slow (due to a lack of system resources).\n\nYou're in luck, as installing the Ollama app is as easy as installing any app on either MacOS or Windows. You simply point your browser to the Ollama download page, download the app for your OS, double-click the downloaded file, and follow the directions. For example, on MacOS, you drag the Ollama app icon into the Applications folder, and you're done.\n\nUsing Ollama is equally easy: select the model you want, let it download, then query away.\n\nPulling an LLM is as easy as selecting it from the list and letting the app do its thing.\n\nIf you've been looking for a reason to try local AI, now is the perfect time.\n\nAlso:\u00a0I tried Sanctum's local AI app, and it's exactly what I needed to keep my data private\n\nThe Ollama app makes migrating away from cloud-based AI as easy as it can get. The app is free to install and use, as are the LLMs in the Ollama library. Give this a chance, and see if it doesn't become your go-to AI tool.\n\nWant more stories about AI? Check out AI Leaderboard, our weekly newsletter.",
        "Link": "https://www.zdnet.com/article/my-go-to-llm-tool-just-dropped-a-super-simple-mac-and-pc-app-for-local-ai-why-you-should-try-it/",
        "Summary": "Ollama is a tool that allows you to run different Large Language Model (LLM) models. The Ollama app is as easy to use as any cloud-based AI interface on the market. It's free to use for MacOS and Windows, and there's no Linux version of the tool. The app also seems to be a bit faster than Msty (in both opening and responding to queries), which is a good thing because local AI can often be a a bit slow (due to a lack of system resources)"
    },
    {
        "Title": "I tested 3 text-to-speech AI models to see which is best - hear my results",
        "Article": "Synthetic voices generated by artificial intelligence are, for better or worse, becoming commonplace. Meanwhile, the number of companies developing this technology is growing rapidly.\n\nRecent innovations in AI, such as the transformer architecture -- which forms the backbone of many generative AI tools, including large language models, generative adversarial networks (GANs), and diffusion models -- have led to the rise of AI systems that can convert text prompts into natural-sounding artificial speech. There are now a wide variety of these text-to-speech (TTS) systems available, each with its particular benefits and shortcomings.\n\nTo gain a clearer sense of which are the most advanced, I tested three of the most popular free TTS tools currently on the market.\n\nElevenLabs is widely considered an industry leader in voice realism, and I found this to be a reasonably accurate assessment in my own experiments with the company's TTS tool. But that realism feels more closely aligned with the voice of a trained voice actor or professional podcaster than it does with ordinary human conversation -- it's almost a little too polished. In that sense, however, it tends to be the preferred choice for many businesses and professionals looking for reliable automated narration. It also supports more than 20 languages, further expanding the platform's reach and appeal.\n\nThe company also released a new text-to-speech model called v3 as a research preview last month. It supports more than 70 languages, and users can spice up their AI-generated dialogue with audio tags that cause it to laugh, sigh, or speak in a whisper, to name just a few examples.\n\nAlso: ElevenLabs' new AI voice assistant can automate your favorite tasks -- and you can try it for free\n\nYou can sign up for a free account with ElevenLabs, and you'll automatically receive 10,000 free credits. Select the \"Text to Speech\" option under \"Playground\" in the left-hand menu, and you'll be redirected to a page where you can enter a custom prompt you'd like the AI system to narrate, select from a range of custom voices, and adjust parameters like speed and stability. Prompts are limited to 5,000 characters, and every character in each iteration of a voice generation uses a single credit.\n\nHume AI's TTS model is another contender for the most realistic voice-generating tool. The company has positioned its proprietary Empathic Voice Interface (EVI) as an AI system that can capture and simulate the subtleties of human speech, imbuing it with a deeper layer of believability. Like ElevenLabs, Hume offers a broad set of premade AI voice characters, each with its own expressive quirks. You can also generate custom voices by describing them in natural-language prompts.\n\nTo test it out, I did my best to describe the voice of Samwise Gamgee from \"The Lord of the Rings,\" as portrayed in the films by Sean Astin. My prompt: \"Gentle but brave hobbit, with a working-class, West Country British -- possibly with a hint of Welsh -- accent. He should sound frightened but resolved to complete his mission.\"\n\nAlso:\u00a0This new text-to-speech AI model understands what it's saying - how to try it for free\n\nAfter I prompted it to say a famous line from the film, \"If I take one more step, it'll be the furthest away from home I've ever been,\" it produced three samples, varying in tone and emphasis. All of them were impressive; to my ear, they contained a degree of realism and emotional depth that isn't replicable by its competitors. They didn't sound much like Astin's Sam, but that was undoubtedly a reflection of the admittedly imperfect description I used as a prompt.\n\nYou can also pepper pauses by adding \"[pause]\" into your prompt, or add slangy infusions like \"y'all\" to enhance the believability of your custom voices.\n\nIf you're looking for an AI voice-generating tool that offers a range of editing features, Descript is the one to choose.\n\nThe company's TTS model generates audio files in a waveform format, which you can edit just as you would in Adobe Audition or a similar platform. You can choose from a library of premade AI voices or submit a short recording of your own voice, and the system will clone it for you.\n\nI tested the voice-cloning feature by asking the system to read a short prompt: \"Summers in New York City are getting brutal, and I need to invest in more high-quality air conditioning.\" (Which is true.) The first time around, the AI-generated version of my voice definitely sounded like me, but there was also a mechanical quality that detracted from the realism.\n\nI decided to give it another try and re-record my voice, this time taking off my Bluetooth headphones and reading the script more slowly and deliberately. The results this time were much more realistic -- a more convincing simulation of my voice, in my opinion, than a similar voice-cloning feature offered by Hume.\n\nAlso: I spoke with an AI version of myself, thanks to Hume's free tool - how to try it\n\nYou can also adjust each piece of AI-generated audio by directly editing your written prompt. It wasn't perfect, of course; my close friends and family members would probably be able to spot the difference, but it would likely fool my more distant acquaintances. I can easily imagine using the tool to narrate my own articles or for some similar use case.\n\nFor podcasters and other content creators looking to quickly polish their audio recordings, Descript also offers an AI feature that identifies and eliminates filler words, unnecessary pauses, \"umms\" and \"uhhs,\" and other unwanted bits of audio.\n\nIt's important to bear in mind that these are just three of a huge number of TTS models currently available, and that each user will have their own preferences based on their professional role, tech savviness, budget, and so on. Before you choose a platform and run with it, spend a few minutes playing with different options to see which user interfaces feel most intuitive and which ones offer features that align most closely with your creative goals. Also remember that services vary in how they use your data.\n\nAlso: Text-to-speech with feeling - this new AI model does everything but shed a tear\n\nRegardless of which platform you end up using, keep your eye on the speed at which this technology continues to evolve. Very soon, we'll likely be living in a world filled with AI voices -- and some of them could sound just like your own.\n\nWant more stories about AI? Check out AI Leaderboard, our weekly newsletter.",
        "Link": "https://www.zdnet.com/article/i-tested-3-text-to-speech-ai-models-to-see-which-is-best-hear-my-results/",
        "Summary": "There are now a wide variety of text-to-speech (TTS) systems available. ElevenLabs is widely considered an industry leader in voice realism. Hume offers a broad set of premade AI voice characters, each with its own expressive quirks. Descript is the one to choose if you're looking for an AI voice-generating tool that offers a range of editing features, such as GIFs and audio tags. The company's V3 model generates audio files in a waveform format, which you can edit just you."
    },
    {
        "Title": "ChatGPT can no longer tell you to break up with your boyfriend",
        "Article": "As OpenAI prepares to drop one of the biggest ChatGPT launches of the year, the company is also taking steps to make the chatbot safer and more reliable with its latest update.\n\nAlso: Could Apple create an AI search engine to rival Gemini and ChatGPT? Here's how it could succeed\n\nOn Monday, OpenAI published a blog post outlining how the company has updated or is updating the chatbot to be more helpful, providing you with better responses in times when you need support, or encouraging a break when you use it too much:\n\nWe build ChatGPT to help you thrive in the ways you choose \u2014 not to hold your attention, but to help you use it well. We\u2019re improving support for tough moments, have rolled out break reminders, and are developing better life advice, all guided by expert input.\u2026\n\nIf you have ever tinkered with ChatGPT, you are likely familiar with the feeling of getting lost in the conversation. Its responses are so amusing and conversational that it is easy to keep the back-and-forth volley going. This is especially true for fun tasks, such as creating an image and then modifying it to generate different renditions that meet your exact needs.\n\nTo encourage a healthy balance and give you more control of your time, ChatGPT will now gently remind you during long sessions to take breaks, as seen in the photo above. OpenAI said it will continue to tune the notification to be helpful and feel more natural.\n\nPeople have been increasingly turning to ChatGPT for advice and support due to several factors, including its conversational capabilities, its availability on demand, and the comfort of receiving advice from an entity that does not know or judge you. OpenAI is aware of this use case. The company has added guardrails to help deal with hallucinations or prevent a lack of empathy and awareness.\n\nFor example, OpenAI recognizes that the GPT-4o model fell short in recognizing signs of delusion or emotional dependency. However, the company continues to develop tools to detect signs of mental or emotional distress, allowing ChatGPT to respond appropriately and providing the user with the best resources.\n\nAlso:\u00a0OpenAI's most capable models hallucinate more than earlier ones\n\nChatGPT is also rolling out a new behavior for high-stakes personal decisions soon. When approached with big personal questions, such as \"Should I break up with my boyfriend?\", the technology will help the user think through their options instead of providing quick answers. This approach is similar to ChatGPT Study Mode, which, as I explained recently, guides users to answers through a series of questions.\n\nOpenAI is working closely with experts, including 90 physicians in over 30 countries, psychiatrists, and human-computer interaction (HCI) researchers, to improve how the chatbot interacts with users in moments of mental or emotional distress. The company is also convening an advisory group of experts in mental health, youth development, and HCI.\n\nEven with these updates, it is crucial to remember that AI is prone to hallucinations, and entering sensitive data has privacy and security implications. OpenAI CEO Sam Altman raised\u00a0privacy concerns\u00a0when inputting sensitive information into ChatGPT in a recent\u00a0interview\u00a0with podcaster Theo Von.\n\nAlso:\u00a0Anthropic wants to stop AI models from turning evil - here's how\n\nTherefore, a healthcare provider is still the best option for your mental health needs.",
        "Link": "https://www.zdnet.com/article/chatgpt-can-no-longer-tell-you-to-break-up-with-your-boyfriend/",
        "Summary": "OpenAI is updating its ChatGPT chatbot to make it safer and more reliable. The company is working closely with experts, including 90 physicians in over 30 countries, psychiatrists and human-computer interaction (HCI) researchers, to improve how the chatbot interacts with users in moments of mental or emotional distress.ChatGPT is also rolling out a new behavior for high-stakes personal decisions soon, such as \"Should I break up with my boyfriend?\", the technology will help the user think through their options instead of providing quick answers."
    },
    {
        "Title": "OpenAI returns to its open-source roots with new open-weight AI models, and it's a big deal",
        "Article": "We all know AI relies on open-source software, but most of the big AI companies avoid opening their code or their large language model (LLM) weights. Today, things have changed. OpenAI, the artificial intelligence titan behind ChatGPT, announced a landmark return to its open-source origins.\n\nThe company unveiled two new open-weight language models, gpt-oss-120b and gpt-oss-20b, marking its first public release of freely available AI model weights since GPT-2 in 2019, long before the\u00a0AI hype took over the tech world.\n\nAlso: OpenAI could launch GPT-5 any minute now - what to expect\n\nOpen-weight models enable anyone to download, examine, run, or fine-tune the LLM, and they eliminate the need to rely on remote cloud APIs or expose in-house sensitive data to external services.\n\nOpenAI has not, however, released the training data used for these models because of legal and safety concerns. That will not please open-source AI purists, but developers worldwide are already putting the two models to the test.\n\nThis change contrasts with OpenAI's approach over the past five years. The business has prioritized proprietary releases fueled by massive Microsoft investments and lucrative API deals.\n\nAfter all, you can't hope to become a trillion-dollar AI company without maximizing your profits. On the other hand, open source has consistently demonstrated that when code is developed openly, everyone, including the company that releases the code, benefits.\n\nThe gpt-oss-120b model targets high-performance servers and desktops with beefed-up specifications -- 60 GB of VRAM and multiple GPUs -- while the gpt-oss-20b version is compact enough for most laptops. You can download the models from Hugging Face or GitHub. In both cases, your hardware must run MacOS or Linux specifically, with MacOS 11 Big Sur or later, or Linux with Ubuntu 18.04 or later to run the programs. It could also work on Windows Subsystem for Linux (WSL) 2.0 on high-powered Windows systems.\n\nOpenAI says, \"The gpt-oss-120b model achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3\u2011mini on common benchmarks and can run on edge devices with just 16 GB of memory.\"\n\nAlso: People are using ChatGPT to write their text messages - here's how you can tell\n\nSo, how good is it? AI expert Nate Jones has kicked its tires and reports, \"This one is specifically aimed at retaking American dominance in open-source models now that Llama has dropped the ball. Early tests indicate a higher than usual risk of hallucination, but the power of the model is real and continues to underline how quickly AI is progressing. I'll be watching for how quickly these models get picked up on Hugging Face by developers (who are hard to spin).\"\n\nThe models are licensed under Apache 2.0, one of the most permissive open licenses. This enables enterprises and developers to use, modify, and monetize the technology without restrictive terms, unlike Meta's not-really open-source Llama LLMs.\n\nBoth models employ a mixture-of-experts (MoE) architecture. This offers robust reasoning capabilities while being optimized for efficiency and tool usage. Programmers will be interested in its code execution capabilities, while writers and researchers will find its inclusion of web search as part of their thought process interesting. On the other hand, early reports show very high levels of hallucinations. Additionally, both models are limited to processing text.\n\nAlso: My go-to LLM tool just dropped a super simple Mac and PC app for local AI\n\nWhy has OpenAI made this move? The company explicitly stated that these open releases aim to lower barriers in emerging markets and among smaller organizations.\n\nThe business has also noticed that the Chinese open-source DeepSeek, which was released in January and immediately made waves thanks to its speed, power, and the fact that it was open source. As Altman said shortly after DeepSeek caught everyone's attention in a Reddit \"Ask Me Anything,\" he believes OpenAI has been \"on the wrong side of history\" about open-sourcing its software.\n\nNow, on the eve of the ChatGPT 5 release, OpenAI is on history's right side again.",
        "Link": "https://www.zdnet.com/article/openai-returns-to-its-open-source-roots-with-new-open-weight-ai-models-and-its-a-big-deal/",
        "Summary": "OpenAI has released two new large language model (LLM) weights. The models are free for anyone to download, examine, run, or fine-tune. Early reports show very high levels of hallucinations. Both models are licensed under Apache 2.0, one of the most permissive open licenses, unlike Meta's not-really open-source Llama LLMs. The release is specifically aimed at retaking U.S. dominance in open- source AI, says Nate Jones, an expert on AI."
    },
    {
        "Title": "Google embeds AI agents deep into its data stack - here's what they can do for you",
        "Article": "I am no stranger to hyperbolic claims from tech companies. Anyone who's on the receiving end of a firehose of press announcements related to AI understands. Everything is game-changing, world-changing, the most, the best, yada, yada, yada.\n\nAnd then there's Google. Google is no stranger to hyperbole. But when a company so steeped in data management as part of its core DNA talks about \"fundamental transformation,\" and says that the world is changing because, \"It's being re-engineered in real-time by data and AI,\" we can consider those claims as fairly credible.\n\nAlso: Got 6 hours? This free AI training from Google can boost your resume today\n\nJust in time for Google Cloud Next Tokyo 2025, Google is making a series of announcements that herald a major change in how enterprises manage data.\n\nYasmeen Ahmad, Google's managing director of Data Cloud, says in a blog post, \"The way we interact with data is undergoing a fundamental transformation, moving beyond human-led analysis to a collaborative partnership with intelligent agents.\"\n\nShe calls this agentic shift, which she describes as, \"A new era where specialized AI agents work autonomously and cooperatively to unlock insights at a scale and speed that was previously unimaginable.\"\n\nAlso:\u00a05 ways to successfully integrate AI agents into your workplace\n\nFrom almost any other company, claims like this would seem like just so much hot air. But Google is dropping a series of announcements about new offerings that provide real-world capabilities to data scientists and engineers in pretty tangible support of the claims.\n\nThere's a fairly fine line between AI chatbots and AI agents. Chatbots are conversational, while agents are tools that perform autonomous tasks. Some users employ chatbots to perform tasks, as I did when I used ChatGPT to analyze some business data. Agents, like ChatGPT Agent, use a conversational interface to receive instructions.\n\nA good way to think of agents is as surrogate team members. Perhaps one agent does data normalization (cleaning up data), while another does migration. Each agent does one or more defined tasks using AI capabilities.\n\nAlso:\u00a0Want AI agents to work together? The Linux Foundation has a plan\n\nIn this context, Google is looking at agents that can automate and simplify tasks for data workers, can communicate with each other, and can free professionals from tedious work so they can focus on \"higher-value tasks.\" Google is also trying to get agents to work together in virtual teams.\n\nThere are, of course, questions about whether agents aren't actually freeing up the time of senior professionals, but are instead taking work away from more junior employees. On the other hand, I don't have anyone to do the grunt work when I'm fully immersed in a project. So anything I can hand off to an agent is more time for projects and writing.\n\nWith all these agents running around, traditional databases just aren't up to the task of keeping them fed. Agents do their reasoning or automation processes across silos. They need access to both historical and live data.\n\nClassic data management methods like real-time OLTP (online transaction processing) and deep-dive OLAP (online analytical processing) isolate data too much for AIs to gain insights from trends and current activities.\n\nOne way to help unify all of these capabilities is by enhancing their database offerings. A few years ago, Google added a columnar engine for AlloyDB. AlloyDB is the company's fully managed database service on Google Cloud Platform that focuses on PostgreSQL users, which is ideal for those who require a PostgreSQL-specific solution.\n\nAlso:\u00a0How AI agents can generate $450 billion by 2028 - and what stands in the way\n\nA columnar engine is one where workloads query specific columns of data, reading only the fields needed for analysis. This leads to faster queries and allows for vectorized execution, where operations are applied to an entire column of data at once.\n\nNow, Google is adding a columnar engine to Spanner, its globally distributed, strongly consistent database service that offers high availability and scalability, designed for enterprises needing global reach and high transactional integrity.\n\nThis also adds power to BigQuery, Google's serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for business agility. As the name implies, BigQuery is ideal for those who need to run fast, SQL-like queries on large datasets.\n\nThe company says this new columnar capability in Spanner speeds up analytical queries by something like 200x on live transactional data. With performance like that, we're talking instant responsiveness to real-time situations.\n\nWhen building enterprise-based AI systems, you need agents to make decisions based on real data. Performing real-time actions based on hallucinated data can get ugly very quickly. This is where RAG (retrieval augmented generation) comes in. Essentially, RAG combines large language models with real-time data access.\n\nAlso:\u00a05 ways to be a great AI agent manager, according to business leaders\n\nYou can start to see how vectorizing search in Spanner and BigQuery becomes necessary when you're feeding in real-time data along with historical information. But getting vector search to work efficiently has traditionally been painful. Google is adding adaptive filtering in AlloyDB to automatically maintain vector indexes and optimize for fast queries on live operational data.\n\nGoogle is also introducing autonomous vector embeddings and generation to BigQuery, which automatically prepares and indexes multimodal data for vector search. This is a key step in creating a sort of semantic memory for agents.\n\nThe company is also introducing the ability to run AI queries right inside of BigQuery. This is, ahem, big. Now, BigQuery users can have AI do its magic across giant gobs of structured and unstructured data, ask complex questions (including subjective ones like \"Which customers are frustrated?\"), and get answers directly within existing analytics tools.\n\nIn addition to building a foundation for agentic cooperation and data access, Google is announcing a series of new capabilities that embed agents in their biggest data tools. Let's look at each in turn.\n\nData engineering agent: Built specifically for data engineers, this agent within BigQuery can simplify and automate complex data pipelines. The entire workflow can be driven by natural-language prompts, from data ingestion to transformations to data-quality assessment to normalization.\n\nSpanner migration agent: Related to the data engineering agent, the Spanner migration agent can simplify data migration from legacy systems to BigQuery. This sort of migration is normally exceptionally tedious and potentially dangerous, but now the agent can do most of the heavy lifting.\n\nData science agent: Data scientists focus on analyzing and interpreting complex data, while data engineers focus on data infrastructure. According to Google, the new data science agent \"triggers entire autonomous analytical workflows, including exploratory data analysis, data cleaning, featurization, machine-learning predictions, and much more. It creates a plan, executes the code, reasons about the results, and presents its findings, all while allowing you to provide feedback and collaborate in sync.\"\n\nCode interpreter: Built as an enhancement of the conversational analytics agent introduced last year, the code interpreter takes in business-analysis questions and converts them to Python code to prepare custom analysis for users. This all runs within Google Data Cloud and uses the Google Data Cloud security infrastructure. It also includes an API available for developers to incorporate conversational analytics agent and code interpreter capabilities in custom code.\n\nAs part of this big series of announcements, Google is introducing an extension to Gemini CLI called Gemini CLI GitHub Actions.\n\nCLI stands for command line interface, basically a terminal interface to your computer. Even though most users left the terminal behind when MS-DOS migrated to Windows, coders to this day make heavy use of the command line. Working in terminal mode lets coders add tools and control the coding process much faster than when they have to find and select items from menus and icons.\n\nAlso:\u00a0Bad vibes: How an AI agent coded its way to disaster\n\nLast month, when Google introduced Gemini CLI, it basically made the features of the Gemini chatbot available in the terminal. Now, Google has extended that capability, providing some agentic features within the terminal environment.\n\nSome of you may be wondering how this compares with Jules, the Google coding agent I wrote about in May. First, Jules works in a secure cloud VM, while Gemini CLI GitHub Actions runs in terminal and integrates with GitHub Actions (the GitHub-based workflow tool).\n\nGoogle says there's a fairly narrow scope to Gemini CLI GitHub Actions compared to Jules. Jules can read your entire codebase, plan and present an approach to a coding challenge, and then execute on it. Gemini CLI GitHub Actions is specifically targeted to intelligent issue triage, accelerated pull-request reviews, and on-demand collaboration.\n\nAlso:\u00a0Most developers use AI in their daily workflows - but they don't trust it, study finds\n\nThe issue-triage capability helps coders manage specific bug reports and feature requests. Pull requests are the way GitHub asks coders to confirm integrating coding changes into branches and master codebases. On-demand collaboration is essentially setting up a chat session whenever you want to talk about your code.\n\nI could easily see a programmer use both. Jules would be great for bigger projects and larger swings, and Gemini CLI GitHub Actions would work well for quicker updates and fixes.\n\nWhat do you think about the agentic shift Google is promoting? Have you started integrating intelligent agents into your own workflows? Which of Google's new data tools or capabilities intrigues you most -- the data engineering agent, the in-query AI reasoning, or something else? Do you see agents as helping senior professionals, replacing junior roles, or both? And how do you feel about running AI workflows directly in BigQuery? Let us know in the comments below.\n\nAlso:\u00a0AI agents will change work and society in internet-sized ways, says AWS VP\n\nWant more stories about AI? Check out\u00a0AI Leaderboard, our weekly newsletter.\n\nYou can follow my day-to-day project updates on social media. Be sure to subscribe to my weekly update newsletter, and follow me on Twitter/X at @DavidGewirtz, on Facebook at Facebook.com/DavidGewirtz, on Instagram at Instagram.com/DavidGewirtz, on Bluesky at @DavidGewirtz.com, and on YouTube at YouTube.com/DavidGewirtzTV.",
        "Link": "https://www.zdnet.com/article/google-embeds-ai-agents-deep-into-its-data-stack-heres-what-they-can-do-for-you/",
        "Summary": "Google is making a series of announcements that herald a major change in how enterprises manage data. The company says the world is changing because, \"It's being re-engineered in real-time by data and AI\" The company is looking at agents that can automate and simplify tasks for data workers, can communicate with each other, and can free professionals from tedious work so they can focus on \"higher-value tasks\" Google is also trying to get agents to work together in virtual teams."
    },
    {
        "Title": "Anthropic's powerful Opus 4.1 model is here - how to access it (and why you'll want to)",
        "Article": "In May, Anthropic released Claude Opus 4, which the company dubbed its most powerful model yet and the best coding model in the world. Only three months later, Anthropic is upping the ante further by launching the highly anticipated Claude Opus 4.1, which now takes its predecessor's crown as Anthropic's most advanced model.\n\nThe Opus family of models is the company's most advanced, intelligent AI models geared toward tackling complex problems. As a result, Claude Opus 4.1, released on Tuesday, excels at those tasks and can even one-up its predecessor on agentic tasks, real-world coding, and reasoning, according to Anthropic.\n\nThe model also comes as the industry is expecting the launch of OpenAI's GPT-5 soon.\n\nAlso: OpenAI could launch GPT-5 any minute now - what to expect\n\nToday we're releasing Claude Opus 4.1, an upgrade to Claude Opus 4 on agentic tasks, real-world coding, and reasoning. pic.twitter.com/25vh0b3FsX\n\nOne of the most impressive use cases of Claude Opus 4 was its performance on the SWE-bench Verified, a human-filtered subset of the SWE-bench, a benchmark that evaluates LLMs' abilities to solve real-world software engineering tasks sourced from GitHub. Claude Opus 4's performance on the SWE-bench Verified supported the claim that it was the \"best coding model in the world.\" As seen in the post above, Opus 4.1 performed even higher.\n\nClaude Opus 4.1 also swept its preceding models across the benchmark board, including the MMMLU, which tests for multilingual capabilities; AIME 2025, which tests for rigor on high school match competition questions; GPQA, which tests for performance on graduate-level reasoning prompts; and more. When pinned against competitors' reasoning models, including OpenAI o3 and Gemini 2.5 Pro, it outperforms them in various benchmarks, including SWE-bench Verified.\n\nWith the release, Anthropic also posted its system card, which delineates all of the safety assessments and evaluations it conducted on the model, as well as its weaknesses, risks, and limitations. A quick overview of the 22-page document shows that the model was deployed with an AI Safety Level 3 (ASL-3) Standard under Anthropic's Responsible Scaling Policy (RSP), and it is still prone to most of the same vulnerabilities.\n\nIf you want to try the model for yourself, it is now available to everyone via the paid Claude plans, which include Claude Pro for $20 per month and Claude Max for $100 per month. It is available in Claude Code, the API, Amazon Bedrock, and Google Cloud's Vertex AI.\n\nGet the morning's top stories in your inbox each day with our\u00a0Tech Today newsletter.",
        "Link": "https://www.zdnet.com/article/anthropics-powerful-opus-4-1-model-is-here-how-to-access-it-and-why-youll-want-to/",
        "Summary": "Claude Opus 4.1 is Anthropic's most advanced, intelligent AI model. It can outperform its predecessor on agentic tasks, real-world coding, and reasoning. The release comes as the industry is expecting the launch of OpenAI's GPT-5 soon. The model is now available to everyone via the paid Claude plans, which include Claude Pro for $20 per month and Claude Max for $100 per month. It is available in Claude Code, the API, Amazon Bedrock, and Google Cloud's Vertex AI."
    },
    {
        "Title": "OpenAI could launch GPT-5 any minute now - what to expect",
        "Article": "Despite OpenAI just launching its highly anticipated open-source models on Tuesday, people are already on the lookout for OpenAI's next big move, with rumblings of an even bigger release on the near horizon: GPT-5.\n\nAlso: ChatGPT can no longer tell you to break up with your boyfriend\n\nWhile speculation about GPT-5 isn't new, there have been easter eggs insinuating that the release might be as early as this week. Boris Power, head of applied research at OpenAI, made an X post on Monday that read: \"Excited to see how the public receives GPT-5!\"\n\nOther companies have been adding to the rumors; for example, Flowith already teased that GPT-5 is coming soon under its model picker. However, the most obvious evidence is OpenAI CEO Sam Altman's X post in which he shared there was \"something big-but-small today,\" a reference to the release of the open models, gpt-oss-120b & gpt-oss-20b, and \"then a big upgrade later this week,\" an almost definite nod to GPT-5.\n\nwe have a lot of new stuff for you over the next few days!something big-but-small today.and then a big upgrade later this week.\n\nSo, why has the possible release of the model been making headlines for months? Here's everything you need to know, updated with the latest information as we know it.\n\nAlso: How to use ChatGPT: A beginner's guide to the most popular AI chatbot\n\nWhen ChatGPT first rose to popularity in November 2022, it was running on GPT-3.5. OpenAI then released GPT-4 in March 2023, which at the time marked a big leap in intelligence. Then, in March 2024, it introduced\u00a0GPT-4o, improving its capabilities nearly across the board and introduced multimodality.\n\nNow, a bit over a year later, we are overdue for GPT-5, and that wasn't always the plan.\n\nAlso: Is ChatGPT Plus really worth $20 when the free version offers so many premium features?\n\nIn February, Altman shared a roadmap on X for the new company models, and in the comments, he said it would be a matter of \"weeks/months.\" However, in April, Altman said there had been a change of plans, pivoting to a release of o3 and o4-mini and pushing GPT-5 back a couple of months. Altman claimed the reason for the delay was due to the challenges encountered when trying to integrate all of the different elements into the model.\n\n(Disclosure: Ziff Davis, ZDNET's parent company, filed an April 2025 lawsuit against OpenAI, alleging it infringed Ziff Davis copyrights in training and operating its AI systems.)\n\nMost recently, in mid-July, OpenAI's CEO, Sam Altman, said GPT-5 was \"releasing soon\" on X. This was followed by a report from The Verge citing insider sources that said GPT-5 could be out in early August. As with most of OpenAI's model releases, the report said the release would be paired with mini and nano versions, which will also be made available to developers through an API.\n\nSo, what exactly will GPT-5 do?\n\nAccording to Altman's initial roadmap, GPT-5 combines the reasoning capabilities found in the o-series models (think o3 and o4 reasoning models) and GPT-series models. The system should understand when to best use the different strengths of both types of models, which would be a huge win for users, because it ensures that the user gets the best combination of speed, cost, and quality.\n\nWhile reasoning models are good at producing high-quality answers for complex problems, they take longer because they are thinking through the problem, are more expensive as a result, and the efforts aren't always necessary if the task doesn't demand it.\n\nAlso:\u00a0This one feature could make GPT-5 a true game changer (if OpenAI gets it right)\n\nOn the other hand, if ChatGPT or the user always opts for the GPT models by default, there are lost opportunities for the reasoning models to produce better responses. Right now, users have the option to toggle, but oftentimes people don't fully know which model would be better suited to their task.\n\nLast week, Nick Turley, head of ChatGPT, told me the goal of this release was \"that the average person does not need to think about which model to use.\" In terms of a timeline, Turley also threw around the term \"soon.\"\n\nIn that same roadmap, Altman said that ChatGPT free users will get unlimited chat access to GPT-5 at the standard intelligence setting, while Plus subscribers could access a higher level of intelligence. Pro subscribers would be able to use it at the highest level. The models are slated to combine all of ChatGPT's best features, including Voice, Canvas, Search, Deep Research, and more.\n\nAlso: How ChatGPT actually works (and why it's been so game-changing)\n\nTo temper expectations a bit, last week Altman reminded users that the model will be experimental and incorporate different research techniques, but it won't be International Math Olympiad (IMO) gold level, an award given to another, still-anonymous model that isn't expected for release for months.\n\nWant more stories about AI? Check out AI Leaderboard, our weekly newsletter.\n\nYou can keep up with my latest stories and tech adventures on social media. Follow me on Twitter/X at @sabrinaa_ortiz and on Instagram at @sabrinaa.ortiz.",
        "Link": "https://www.zdnet.com/article/openai-could-launch-gpt-5-any-minute-now-what-to-expect/",
        "Summary": "GPT-5 is expected to be released later this year. It will be available in two versions: 1.5 and 2.5. The new version will be free to download and use for a year. The 2.0 version of the software will cost $99.99 for the full version and $49.99 with the upgrade. It is the latest in a long line of software upgrades for the popular chat app. The original version of this article was published on June 7, 2013. We are happy to make clear that the release date of the next version of GPT is scheduled for August."
    }
]